{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe065b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/sequence_similarity_search/classes')\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/fast-soft-sort/fast_soft_sort')\n",
    "from data_classes import BERTDataset\n",
    "from dist_perm import DistPerm\n",
    "import utils\n",
    "from pytorch_ops import soft_rank\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "class AnchorNet(nn.Module):\n",
    "    def __init__(self, num_anchs, d, k):\n",
    "        super(AnchorNet, self).__init__()\n",
    "        self.anchors = nn.Linear(d, num_anchs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, data, query):\n",
    "        data_rank = torch.clamp(soft_rank(self.anchors(data), direction=\"DESCENDING\", regularization_strength=.001), max=k)\n",
    "#         data_rank = soft_rank(self.anchors(data), direction=\"DESCENDING\")\n",
    "        \n",
    "        query_rank = torch.clamp(soft_rank(self.anchors(query), direction=\"DESCENDING\", regularization_strength=.001), max=k)\n",
    "#         query_rank = soft_rank(self.anchors(query), direction=\"DESCENDING\")\n",
    "#         print(query_rank)\n",
    "        \n",
    "        out = self.softmax(torch.matmul(query_rank, data_rank.T))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b112337",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "D = 128\n",
    "num_queries = 3200\n",
    "num_anchors = 128\n",
    "R = 100\n",
    "k = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaecf3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([685571, 128])\n",
      "torch.Size([100, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "file_q = '../datasets/Q.pt'\n",
    "file_db = '../datasets/D.pt'\n",
    "data_source = BERTDataset(file_q, file_db, n)\n",
    "db = data_source.generate_db()\n",
    "data = np.array(db).astype(np.float32)\n",
    "queries = data_source.generate_queries(num_queries)\n",
    "quers = np.array(queries).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a602d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_l2 = NearestNeighbors()\n",
    "index_l2.fit(data)\n",
    "true = torch.tensor(index_l2.kneighbors(quers, n_neighbors=1)[1])\n",
    "truth = torch.nn.functional.one_hot(true.squeeze(), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0c7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "def dataset_split(dataset, train_frac):\n",
    "    length = len(dataset)\n",
    "\n",
    "    # Use int to get the floor to favour allocation to the smaller valid and test sets    \n",
    "    train_length = int(length * train_frac)\n",
    "    valid_length = int((length - train_length) / 2)\n",
    "    test_length  = length - train_length - valid_length\n",
    "\n",
    "    sets = random_split(dataset, (train_length, valid_length, test_length))\n",
    "    dataset = {name: set for name, set in zip(('train', 'valid', 'test'), sets)}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223dcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "train_split = .8 \n",
    "\n",
    "query_data = []\n",
    "for i in range(queries.shape[0]):\n",
    "    query_data.append([queries[i], true[i]])\n",
    "# print(len(train_data))\n",
    "# print(np.array(train_data).shape)\n",
    "# db_datasets = dataset_split(db, train_split)\n",
    "query_datasets = dataset_split(query_data, train_split)\n",
    "query_loader = torch.utils.data.DataLoader(dataset=query_datasets['train'], \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "# t = [queries, true]\n",
    "# print(torch.tensor(t))\n",
    "# print(true.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=query_datasets['test'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "model = AnchorNet(num_anchors, D, k)\n",
    "# # print(model.forward(db,queries).shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)  \n",
    "\n",
    "\n",
    "\n",
    "# DataLoader(db_datasets[\"train\"], batch_size=10000)\n",
    "# DataLoader(db_datasets[\"train\"], batch_size=10000)\n",
    "# DataLoader(db_datasets[\"train\"], batch_size=10000)\n",
    "\n",
    "# index_dp = DistPerm(num_anchors, k=k, dist='dot')\n",
    "# index_dp.fit(db)\n",
    "# dp_hashes = index_dp.add(db)\n",
    "# found_dp = index_dp.search(queries, R)\n",
    "# found_dp = found_dp.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd0ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    for i, (q, l) in enumerate(query_loader):  \n",
    "#         print(q.shape)\n",
    "#         print(q)\n",
    "#         break\n",
    "        outputs = model(db,queries)\n",
    "        loss = criterion(outputs, true.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#     print('Loss: {:.4f}'.format(loss.item()))\n",
    "#     break\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for q, l in test_loader:\n",
    "                outputs = model(db,q)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "#                 print(predicted.shape)\n",
    "                total += l.size(0)\n",
    "#                 print(l.size(0))\n",
    "                correct += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "            print ('Epoch [{}], Loss: {:.4f}, recall_test: {:.4f}'\n",
    "                   .format(epoch+1, loss.item(), correct / total))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_weights = model.anchors.state_dict()['weight']\n",
    "print(lin_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501affaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soft_rank(model.anchors(queries), regularization_strength=.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a66923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
