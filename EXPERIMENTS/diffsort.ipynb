{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ddb0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/sequence_similarity_search/classes')\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/fast-soft-sort/fast_soft_sort')\n",
    "from data_classes import BERTDataset\n",
    "from dist_perm import DistPerm\n",
    "import utils\n",
    "from pytorch_ops import soft_rank\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "class AnchorNet(nn.Module):\n",
    "    def __init__(self, num_anchs, d, k):\n",
    "        super(AnchorNet, self).__init__()\n",
    "        self.anchors = nn.Linear(d, num_anchs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, data, query):\n",
    "        data_rank = torch.clamp(soft_rank(self.anchors(data), direction=\"DESCENDING\", regularization_strength=.001), max=k)\n",
    "        query_rank = torch.clamp(soft_rank(self.anchors(query), direction=\"DESCENDING\", regularization_strength=.001), max=k)\n",
    "        out = self.softmax(torch.matmul(query_rank, data_rank.T))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9db55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "D = 128\n",
    "num_queries = 3200\n",
    "num_anchors = 128\n",
    "R = 100\n",
    "k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c0c7076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([685571, 128])\n",
      "torch.Size([100, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "file_q = '../datasets/Q.pt'\n",
    "file_db = '../datasets/D.pt'\n",
    "data_source = BERTDataset(file_q, file_db, n)\n",
    "db = data_source.generate_db()\n",
    "data = np.array(db).astype(np.float32)\n",
    "queries = data_source.generate_queries(num_queries)\n",
    "quers = np.array(queries).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "62bcac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dataset_split(dataset, train_frac):\n",
    "    length = len(dataset)\n",
    "   \n",
    "    train_length = int(length * train_frac)\n",
    "    valid_length = int((length - train_length) / 2)\n",
    "    test_length  = length - train_length - valid_length\n",
    "\n",
    "    sets = random_split(dataset, (train_length, valid_length, test_length))\n",
    "    dataset = {name: set for name, set in zip(('train', 'valid', 'test'), sets)}\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d2e9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits fine in mem\n",
    "batch_size=3200\n",
    "train_split = .8 \n",
    "\n",
    "query_datasets = dataset_split(quers, train_split)\n",
    "doc_datasets = dataset_split(db, train_split)\n",
    "\n",
    "# The fixed train queries\n",
    "query_data_loader = torch.utils.data.DataLoader(dataset=query_datasets['train'], \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "# The fixed test queries \n",
    "query_data_test_loader = torch.utils.data.DataLoader(dataset=query_datasets['test'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# The train docs\n",
    "docs_loader = torch.utils.data.DataLoader(dataset=doc_datasets['train'], \n",
    "                                           batch_size=10000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# The test docs\n",
    "docs_test_loader = torch.utils.data.DataLoader(dataset=doc_datasets['test'], \n",
    "                                           batch_size=10000, \n",
    "                                           shuffle=False)\n",
    "\n",
    "model = AnchorNet(num_anchors, D, k)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e6ebd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the nearest neighbors for each batch of docs \n",
    "# d = docs\n",
    "# ret = train/test\n",
    "def return_loader(d, ret):\n",
    "    index_l2 = NearestNeighbors()\n",
    "    index_l2.fit(d)\n",
    "    q = None\n",
    "#   Get the right data\n",
    "    if ret=='query':\n",
    "        q = next(iter(query_data_loader))\n",
    "    elif ret=='test':\n",
    "        q = next(iter(query_data_test_loader))\n",
    "#   Get the true nearest neighbors\n",
    "    true = torch.tensor(index_l2.kneighbors(q, n_neighbors=1)[1])\n",
    "    query_data = []\n",
    "    for i in range(q.shape[0]):\n",
    "        query_data.append([q[i], true[i]])\n",
    "    test_set = dataset_split(query_data, 1)\n",
    "#   Return data as a dataloader\n",
    "    data = torch.utils.data.DataLoader(dataset=test_set['train'], \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46ad9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.9974, recall_test: 0.2156\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for d in docs_test_loader:\n",
    "        test_loader = return_loader(d, ret='test')\n",
    "        for q, l in test_loader:\n",
    "#             print(q)\n",
    "            outputs = model(d,q)\n",
    "            test_loss = criterion(outputs, l.squeeze())\n",
    "#           Recall: TP / (TP + TN)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += l.size(0)\n",
    "            correct += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "        print ('Test Loss: {:.4f}, recall_test: {:.4f}'\n",
    "               .format(test_loss.item(), correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "583566b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1450e-01,  4.7607e-02,  7.6660e-02,  ..., -1.4648e-03,\n",
      "          6.3293e-02,  6.4758e-02],\n",
      "        [-1.3733e-04,  2.6465e-01, -5.5969e-02,  ..., -2.3499e-02,\n",
      "          5.6030e-02, -1.1096e-01],\n",
      "        [ 1.0323e-02, -3.3813e-02,  1.1823e-01,  ..., -1.1328e-01,\n",
      "          6.9885e-03,  5.8105e-02],\n",
      "        ...,\n",
      "        [ 8.0688e-02, -1.6266e-02, -1.6089e-01,  ..., -8.8257e-02,\n",
      "         -5.4016e-03,  4.3030e-02],\n",
      "        [-9.3689e-03, -1.9211e-02, -1.8402e-02,  ...,  8.1482e-02,\n",
      "         -8.1299e-02, -1.7395e-01],\n",
      "        [ 6.7200e-02, -2.7328e-02,  7.6294e-03,  ..., -1.0999e-01,\n",
      "         -6.7993e-02, -3.2787e-03]])\n",
      "Epoch [0] Step [0] Loss: 8.6280\n",
      "tensor([[-1.1450e-01,  4.7607e-02,  7.6660e-02,  ..., -1.4648e-03,\n",
      "          6.3293e-02,  6.4758e-02],\n",
      "        [-1.3733e-04,  2.6465e-01, -5.5969e-02,  ..., -2.3499e-02,\n",
      "          5.6030e-02, -1.1096e-01],\n",
      "        [ 1.0323e-02, -3.3813e-02,  1.1823e-01,  ..., -1.1328e-01,\n",
      "          6.9885e-03,  5.8105e-02],\n",
      "        ...,\n",
      "        [ 8.0688e-02, -1.6266e-02, -1.6089e-01,  ..., -8.8257e-02,\n",
      "         -5.4016e-03,  4.3030e-02],\n",
      "        [-9.3689e-03, -1.9211e-02, -1.8402e-02,  ...,  8.1482e-02,\n",
      "         -8.1299e-02, -1.7395e-01],\n",
      "        [ 6.7200e-02, -2.7328e-02,  7.6294e-03,  ..., -1.0999e-01,\n",
      "         -6.7993e-02, -3.2787e-03]])\n",
      "Epoch [0] Step [1] Loss: 8.6402\n",
      "tensor([[-1.1450e-01,  4.7607e-02,  7.6660e-02,  ..., -1.4648e-03,\n",
      "          6.3293e-02,  6.4758e-02],\n",
      "        [-1.3733e-04,  2.6465e-01, -5.5969e-02,  ..., -2.3499e-02,\n",
      "          5.6030e-02, -1.1096e-01],\n",
      "        [ 1.0323e-02, -3.3813e-02,  1.1823e-01,  ..., -1.1328e-01,\n",
      "          6.9885e-03,  5.8105e-02],\n",
      "        ...,\n",
      "        [ 8.0688e-02, -1.6266e-02, -1.6089e-01,  ..., -8.8257e-02,\n",
      "         -5.4016e-03,  4.3030e-02],\n",
      "        [-9.3689e-03, -1.9211e-02, -1.8402e-02,  ...,  8.1482e-02,\n",
      "         -8.1299e-02, -1.7395e-01],\n",
      "        [ 6.7200e-02, -2.7328e-02,  7.6294e-03,  ..., -1.0999e-01,\n",
      "         -6.7993e-02, -3.2787e-03]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4790a65d2dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#   the train queries and the correct data labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mquery_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-9ae779384db6>\u001b[0m in \u001b[0;36mreturn_loader\u001b[0;34m(d, ret)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#   Get the true nearest neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_l2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mquery_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 delayed_query(\n\u001b[1;32m    664\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             )\n\u001b[1;32m    667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "#   Train step: get batch of train documents, then generate the dataloader containing \n",
    "#   the train queries and the correct data labels\n",
    "    for step, d in enumerate(docs_loader):\n",
    "        query_loader = return_loader(d, ret='query')\n",
    "        for i, (q, l) in enumerate(query_loader):  \n",
    "            outputs = model(d,q)\n",
    "            loss = criterion(outputs, l.squeeze())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch [{}] Step [{}] Loss: {:.4f}'.format(epoch, step, loss.item()))\n",
    "        \n",
    "#   Eval step: repeat but with the test documents and test queries      \n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for d in docs_test_loader:\n",
    "                test_loader = return_loader(d, ret='test')\n",
    "                for q, l in test_loader:\n",
    "                    outputs = model(d,q)\n",
    "                    test_loss = criterion(outputs, l.squeeze())\n",
    "#                   Recall: TP / (TP + TN)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += l.size(0)\n",
    "                    correct += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "                print ('Epoch [{}], Loss: {:.4f}, Test Loss: {:.4f}, recall_test: {:.4f}'\n",
    "                       .format(epoch+1, loss.item(), test_loss.item(), correct / total))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b81b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
