{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddb0015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/sequence_similarity_search/classes')\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/fast-soft-sort/fast_soft_sort')\n",
    "from data_classes import BERTDataset\n",
    "from dist_perm import DistPerm\n",
    "import utils\n",
    "import pytorch_ops\n",
    "# import torchsort\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def rank(array):\n",
    "    if not torch.cuda.is_available():\n",
    "        return pytorch_ops.soft_rank(array.cpu(), direction=\"DESCENDING\", regularization_strength=.001)\n",
    "#     else:\n",
    "#         return torchsort.soft_rank(-1 * array, regularization_strength=.001)\n",
    "\n",
    "class AnchorNet(nn.Module):\n",
    "    def __init__(self, num_anchs, d, k):\n",
    "        super(AnchorNet, self).__init__()\n",
    "        self.anchors = nn.Linear(d, num_anchs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, data, query):\n",
    "        data_rank = torch.clamp(rank(self.anchors(data)), max=k)\n",
    "        query_rank = torch.clamp(rank(self.anchors(query)), max=k)\n",
    "        out = self.softmax(torch.matmul(query_rank, data_rank.T))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "n = 100000\n",
    "D = 128\n",
    "num_queries = 3200\n",
    "num_anchors = 128\n",
    "k = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0c7076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "file_q = './Q.pt'\n",
    "file_db = './D.pt'\n",
    "data_source = BERTDataset(file_q, file_db, n)\n",
    "db = data_source.generate_db()\n",
    "data = np.array(db).astype(np.float32)\n",
    "queries = data_source.generate_queries(num_queries)\n",
    "quers = np.array(queries).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bcac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for splitting into datasets \n",
    "\n",
    "import torch, itertools\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dataset_split(dataset, train_frac):\n",
    "    length = len(dataset)\n",
    "   \n",
    "    train_length = int(length * train_frac)\n",
    "    valid_length = int((length - train_length) / 2)\n",
    "    test_length  = length - train_length - valid_length\n",
    "\n",
    "    sets = random_split(dataset, (train_length, valid_length, test_length))\n",
    "    dataset = {name: set for name, set in zip(('train', 'val', 'test'), sets)}\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ebd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the nearest neighbors for each batch of docs. Then combine with the queries \n",
    "# to produce the dataloader for queries containing data and label\n",
    "# d = docs\n",
    "# ret = train/test\n",
    "def return_loader(d, ret):\n",
    "    index_l2 = NearestNeighbors()\n",
    "    index_l2.fit(d)\n",
    "    q = None\n",
    "#   Get the right data\n",
    "    if ret=='query':\n",
    "        q = next(iter(query_data_loader))\n",
    "    elif ret=='test':\n",
    "        q = next(iter(query_data_test_loader))\n",
    "    elif ret=='val':\n",
    "        q = next(iter(query_data_val_loader))\n",
    "#   Get the true nearest neighbors\n",
    "    true = torch.tensor(index_l2.kneighbors(q, n_neighbors=1)[1])\n",
    "    query_data = []\n",
    "    for i in range(q.shape[0]):\n",
    "        query_data.append([q[i], true[i]])\n",
    "    test_set = dataset_split(query_data, 1)\n",
    "#   Return data as a dataloader\n",
    "    data = torch.utils.data.DataLoader(dataset=test_set['train'], \n",
    "                                           batch_size=320, \n",
    "                                           shuffle=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2e9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits fine in mem\n",
    "batch_size=3200\n",
    "train_split = .8\n",
    "\n",
    "query_datasets = dataset_split(quers, train_split)\n",
    "# doc_datasets = dataset_split(db, train_split)\n",
    "doc_datasets = dataset_split(db, train_split)\n",
    "\n",
    "# The fixed train queries\n",
    "query_data_loader = torch.utils.data.DataLoader(dataset=query_datasets['train'], \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "# The fixed test queries \n",
    "query_data_test_loader = torch.utils.data.DataLoader(dataset=query_datasets['test'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# The fixed val queries \n",
    "query_data_val_loader = torch.utils.data.DataLoader(dataset=query_datasets['val'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# The train docs\n",
    "docs_loader = torch.utils.data.DataLoader(dataset=doc_datasets['train'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# The test docs\n",
    "docs_test_loader = torch.utils.data.DataLoader(dataset=doc_datasets['test'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=False)\n",
    "\n",
    "docs_val_loader = torch.utils.data.DataLoader(dataset=doc_datasets['val'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f09e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnchorNet(num_anchors, D, k).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr=.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ad9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1256, recall_test: 0.3547\n"
     ]
    }
   ],
   "source": [
    "# Get the initial performance of random anchors \n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for d in docs_val_loader:\n",
    "    # for d in docs_loader:\n",
    "        test_loader = return_loader(d, ret='val')\n",
    "        d = d.to(device)\n",
    "        for q, l in test_loader:\n",
    "#             print(q)\n",
    "            q = q.to(device)\n",
    "            l = l.to(device)\n",
    "            outputs = model(d,q)\n",
    "            test_loss = criterion(outputs, l.squeeze())\n",
    "#           Recall: TP / (TP + TN)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += l.size(0)\n",
    "            correct += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "    print ('Test Loss: {:.4f}, recall_test: {:.4f}'\n",
    "            .format(test_loss.item(), correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583566b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "for epoch in range(1000):\n",
    "#   Train step: get batch of train documents, then generate the dataloader containing \n",
    "#   the train queries and the correct data labels\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for step, d in enumerate(docs_loader):\n",
    "        query_loader = return_loader(d, ret='query')\n",
    "        d = d.to(device)\n",
    "        for i, (q, l) in enumerate(query_loader):  \n",
    "            q = q.to(device)\n",
    "            l = l.to(device)\n",
    "            outputs = model(d,q)\n",
    "            loss = criterion(outputs, l.squeeze())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += l.size(0)\n",
    "            train_correct += (predicted == l.flatten()).sum().item()\n",
    "        # print('Epoch [{}] Step [{}] Loss: {:.4f}'.format(epoch, step, loss.item()))\n",
    "        \n",
    "#   Eval step: repeat but with the test documents and test queries      \n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            for d in docs_test_loader:\n",
    "            # for d in docs_loader:\n",
    "                test_loader = return_loader(d, ret='test')\n",
    "                d = d.to(device)\n",
    "                for q, l in test_loader:\n",
    "                    q = q.to(device)\n",
    "                    l = l.to(device)\n",
    "                    outputs = model(d,q)\n",
    "                    test_loss = criterion(outputs, l.squeeze())\n",
    "#                   Recall: TP / (TP + TN)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += l.size(0)\n",
    "                    correct += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "            for d in docs_val_loader:\n",
    "            # for d in docs_loader:\n",
    "                test_loader = return_loader(d, ret='val')\n",
    "                d = d.to(device)\n",
    "                for q, l in test_loader:\n",
    "                    q = q.to(device)\n",
    "                    l = l.to(device)\n",
    "                    outputs = model(d,q)\n",
    "#                   Recall: TP / (TP + TN)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += l.size(0)\n",
    "                    correct_val += (predicted == l.flatten()).sum().item()\n",
    "\n",
    "            print ('Epoch [{}], Loss: {:.4f}, Test Loss: {:.4f}, recall_train: {:.4f}, recall_test: {:.4f} recall_val: {:.4f}'\n",
    "                    .format(epoch+1, loss.item(), test_loss.item(), train_correct / train_total, correct / total, correct_val / total_val))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "#     if epoch % 20 == 0:\n",
    "#       torch.save(model.state_dict(), \"/content/gdrive/MyDrive/ckpt/model_epoch_{}_anchor_{}_k_{}_lr_{}.pt\".format(epoch, num_anchors, k, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b81b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
