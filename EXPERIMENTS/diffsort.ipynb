{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddb0015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/sequence_similarity_search/classes')\n",
    "sys.path.insert(1, '/Users/derekhuang/Documents/Research/fast-soft-sort/fast_soft_sort')\n",
    "from data_classes import BERTDataset\n",
    "from dist_perm import DistPerm\n",
    "import utils\n",
    "import pytorch_ops\n",
    "# import torchsort\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def rank(array):\n",
    "    if not torch.cuda.is_available():\n",
    "        return pytorch_ops.soft_rank(array.cpu(), direction=\"DESCENDING\", regularization_strength=.00001)\n",
    "    # else:\n",
    "        # return torchsort.soft_rank(-1 * array, regularization_strength=.00001)\n",
    "\n",
    "class AnchorNet(nn.Module):\n",
    "    def __init__(self, num_anchs, d, k):\n",
    "        super(AnchorNet, self).__init__()\n",
    "        self.anchors = nn.Linear(d, num_anchs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, data, query):\n",
    "        data_rank = torch.clamp(rank(self.anchors(data)), max=k)\n",
    "        query_rank = torch.clamp(rank(self.anchors(query)), max=k)\n",
    "        out = self.softmax(torch.matmul(query_rank, data_rank.T))\n",
    "        return out\n",
    "\n",
    "    def evaluate(self, data, query):\n",
    "        q_dist = self.anchors(query)\n",
    "        d_dist = self.anchors(data)\n",
    "\n",
    "        query_ranks = self.k*torch.ones(q_dist.shape, dtype=torch.float)\n",
    "        data_ranks = self.k*torch.ones(d_dist.shape, dtype=torch.float)\n",
    "\n",
    "        query_ids = torch.argsort(q_dist, dim=1)[:, :self.k]\n",
    "        data_ids = torch.argsort(d_dist, dim=1)[:, :self.k]\n",
    "\n",
    "        q_ids = torch.arange(query.shape[0])[:,None]\n",
    "        d_ids = torch.arange(data.shape[0])[:,None]\n",
    "        query_ranks[q_ids, query_ids] = torch.arange(self.k, dtype=torch.float)\n",
    "        data_ranks[d_ids, data_ids] = torch.arange(self.k, dtype=torch.float)\n",
    "\n",
    "        db_dists = torch.cdist(data_ranks, query_ranks, p=1).float()\n",
    "        closest_idx = torch.topk(db_dists, 10, dim=0, largest=False)\n",
    "        return closest_idx[1].transpose(0,1), query_ranks, closest_idx[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "n = 100000\n",
    "D = 128\n",
    "num_queries = 3200\n",
    "num_anchors = 128\n",
    "k = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0c7076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "file_q = './Q.pt'\n",
    "file_db = './D.pt'\n",
    "data_source = BERTDataset(file_q, file_db, n)\n",
    "db = data_source.generate_db()\n",
    "data = np.array(db).astype(np.float32)\n",
    "queries = data_source.generate_queries(num_queries)\n",
    "quers = np.array(queries).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bcac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for splitting into datasets \n",
    "\n",
    "import torch, itertools\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dataset_split(dataset, train_frac):\n",
    "    length = len(dataset)\n",
    "   \n",
    "    train_length = int(length * train_frac)\n",
    "    valid_length = int((length - train_length) / 2)\n",
    "    test_length  = length - train_length - valid_length\n",
    "\n",
    "    sets = random_split(dataset, (train_length, valid_length, test_length))\n",
    "    dataset = {name: set for name, set in zip(('train', 'val', 'test'), sets)}\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ebd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the nearest neighbors for each batch of docs. Then combine with the queries \n",
    "# to produce the dataloader for queries containing data and label\n",
    "# d = docs\n",
    "# ret = train/test\n",
    "def return_loader(d, ret):\n",
    "    index_l2 = NearestNeighbors()\n",
    "    index_l2.fit(d)\n",
    "    q = None\n",
    "#   Get the right data\n",
    "    if ret=='query':\n",
    "        q = next(iter(query_data_loader))\n",
    "    elif ret=='test':\n",
    "        q = next(iter(query_data_test_loader))\n",
    "    elif ret=='val':\n",
    "        q = next(iter(query_data_val_loader))\n",
    "#   Get the true nearest neighbors\n",
    "    true = torch.tensor(index_l2.kneighbors(q, n_neighbors=1)[1])\n",
    "    query_data = []\n",
    "    for i in range(q.shape[0]):\n",
    "        query_data.append([q[i], true[i]])\n",
    "    test_set = dataset_split(query_data, 1)\n",
    "#   Return data as a dataloader\n",
    "    data = torch.utils.data.DataLoader(dataset=test_set['train'], \n",
    "                                           batch_size=320, \n",
    "                                           shuffle=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2e9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits fine in mem\n",
    "batch_size=3200\n",
    "train_split = .8\n",
    "\n",
    "query_datasets = dataset_split(quers, train_split)\n",
    "# doc_datasets = dataset_split(db, train_split)\n",
    "doc_datasets = dataset_split(db, train_split)\n",
    "\n",
    "# The fixed train queries\n",
    "query_data_loader = torch.utils.data.DataLoader(dataset=query_datasets['train'], \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "# The fixed test queries \n",
    "query_data_test_loader = torch.utils.data.DataLoader(dataset=query_datasets['test'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# The fixed val queries \n",
    "query_data_val_loader = torch.utils.data.DataLoader(dataset=query_datasets['val'], \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# The train docs\n",
    "docs_loader = torch.utils.data.DataLoader(dataset=doc_datasets['train'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# The test docs\n",
    "docs_test_loader = torch.utils.data.DataLoader(dataset=doc_datasets['test'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=False)\n",
    "\n",
    "docs_val_loader = torch.utils.data.DataLoader(dataset=doc_datasets['val'], \n",
    "                                           batch_size=5000, \n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f749027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnchorNet(num_anchors, D, k).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr=.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ad9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1256, recall_test: 0.3547\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for d in docs_test_loader:\n",
    "    # for d in docs_loader:\n",
    "        test_loader = return_loader(d, ret='test')\n",
    "        d = d.to(device)\n",
    "        for q, l in test_loader:\n",
    "#             print(q)\n",
    "            q = q.to(device)\n",
    "            l = l.to(device)\n",
    "            outputs = model.evaluate(d,q)\n",
    "#           Recall: TP / (TP + TN)\n",
    "            predicted = outputs[0][:,0]\n",
    "            total += l.size(0)\n",
    "            correct += (predicted.to(device) == l.flatten()).sum().item()\n",
    "\n",
    "    print ('recall_test: {:.4f}'\n",
    "            .format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583566b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 8.1558, Test Loss: 8.1436, recall_train: 0.3626, recall_test: 0.3391 recall_val: 0.3828\n",
      "Epoch [2], Loss: 8.0732, Test Loss: 8.1298, recall_train: 0.3819, recall_test: 0.3578 recall_val: 0.4484\n",
      "Epoch [3], Loss: 8.0337, Test Loss: 8.1129, recall_train: 0.4188, recall_test: 0.3766 recall_val: 0.4359\n",
      "Epoch [4], Loss: 8.1161, Test Loss: 8.1130, recall_train: 0.4191, recall_test: 0.3781 recall_val: 0.4688\n",
      "Epoch [5], Loss: 8.0201, Test Loss: 8.0792, recall_train: 0.4360, recall_test: 0.4250 recall_val: 0.4547\n",
      "Epoch [6], Loss: 8.0584, Test Loss: 8.1136, recall_train: 0.4333, recall_test: 0.4031 recall_val: 0.4313\n",
      "Epoch [7], Loss: 8.0330, Test Loss: 8.0777, recall_train: 0.4420, recall_test: 0.4172 recall_val: 0.4500\n",
      "Epoch [8], Loss: 8.0429, Test Loss: 8.1260, recall_train: 0.4581, recall_test: 0.3891 recall_val: 0.4625\n",
      "Epoch [9], Loss: 8.0847, Test Loss: 8.1178, recall_train: 0.4570, recall_test: 0.3844 recall_val: 0.4469\n",
      "Epoch [10], Loss: 8.0816, Test Loss: 8.1045, recall_train: 0.4594, recall_test: 0.4031 recall_val: 0.4688\n",
      "Epoch [11], Loss: 8.0071, Test Loss: 8.0775, recall_train: 0.4633, recall_test: 0.4141 recall_val: 0.4828\n",
      "Epoch [12], Loss: 8.0259, Test Loss: 8.0768, recall_train: 0.4699, recall_test: 0.4281 recall_val: 0.4859\n",
      "Epoch [13], Loss: 8.0758, Test Loss: 8.0874, recall_train: 0.4728, recall_test: 0.4141 recall_val: 0.4797\n",
      "Epoch [14], Loss: 7.9924, Test Loss: 8.0779, recall_train: 0.4747, recall_test: 0.4313 recall_val: 0.4688\n",
      "Epoch [15], Loss: 8.0212, Test Loss: 8.0771, recall_train: 0.4830, recall_test: 0.4328 recall_val: 0.4891\n",
      "Epoch [16], Loss: 7.9968, Test Loss: 8.0796, recall_train: 0.4745, recall_test: 0.4484 recall_val: 0.4766\n",
      "Epoch [17], Loss: 8.0230, Test Loss: 8.0841, recall_train: 0.4890, recall_test: 0.4391 recall_val: 0.4734\n",
      "Epoch [18], Loss: 8.0781, Test Loss: 8.0663, recall_train: 0.4873, recall_test: 0.4484 recall_val: 0.4891\n",
      "Epoch [19], Loss: 7.9731, Test Loss: 8.0656, recall_train: 0.4810, recall_test: 0.4500 recall_val: 0.4703\n",
      "Epoch [20], Loss: 7.9948, Test Loss: 8.0223, recall_train: 0.4963, recall_test: 0.4813 recall_val: 0.4859\n",
      "Epoch [21], Loss: 8.0911, Test Loss: 8.0619, recall_train: 0.4973, recall_test: 0.4719 recall_val: 0.4781\n",
      "Epoch [22], Loss: 8.0443, Test Loss: 8.0705, recall_train: 0.4920, recall_test: 0.4578 recall_val: 0.4750\n",
      "Epoch [23], Loss: 8.0080, Test Loss: 8.0557, recall_train: 0.5074, recall_test: 0.4547 recall_val: 0.4766\n",
      "Epoch [24], Loss: 8.0291, Test Loss: 8.0513, recall_train: 0.5060, recall_test: 0.4672 recall_val: 0.4844\n",
      "Epoch [25], Loss: 8.0024, Test Loss: 8.0619, recall_train: 0.5051, recall_test: 0.4625 recall_val: 0.5062\n",
      "Epoch [26], Loss: 8.0251, Test Loss: 8.0324, recall_train: 0.5133, recall_test: 0.4766 recall_val: 0.4969\n",
      "Epoch [27], Loss: 7.9670, Test Loss: 8.0501, recall_train: 0.5083, recall_test: 0.4469 recall_val: 0.5016\n",
      "Epoch [28], Loss: 7.9911, Test Loss: 8.0289, recall_train: 0.5142, recall_test: 0.4625 recall_val: 0.4891\n",
      "Epoch [29], Loss: 7.9504, Test Loss: 8.0197, recall_train: 0.5204, recall_test: 0.4703 recall_val: 0.4969\n",
      "Epoch [30], Loss: 8.0132, Test Loss: 8.0080, recall_train: 0.5106, recall_test: 0.4891 recall_val: 0.5188\n",
      "Epoch [31], Loss: 7.9828, Test Loss: 8.0376, recall_train: 0.5085, recall_test: 0.4641 recall_val: 0.5125\n",
      "Epoch [32], Loss: 8.0229, Test Loss: 7.9992, recall_train: 0.5154, recall_test: 0.4719 recall_val: 0.5062\n",
      "Epoch [33], Loss: 7.9907, Test Loss: 8.0259, recall_train: 0.5161, recall_test: 0.4891 recall_val: 0.5375\n",
      "Epoch [34], Loss: 7.9718, Test Loss: 7.9863, recall_train: 0.5192, recall_test: 0.4969 recall_val: 0.5391\n",
      "Epoch [35], Loss: 7.9802, Test Loss: 8.0436, recall_train: 0.5189, recall_test: 0.4656 recall_val: 0.5156\n",
      "Epoch [36], Loss: 7.9590, Test Loss: 8.0067, recall_train: 0.5251, recall_test: 0.4891 recall_val: 0.5047\n",
      "Epoch [37], Loss: 8.0073, Test Loss: 7.9933, recall_train: 0.5213, recall_test: 0.4859 recall_val: 0.4969\n",
      "Epoch [38], Loss: 7.9773, Test Loss: 7.9879, recall_train: 0.5229, recall_test: 0.5047 recall_val: 0.5000\n",
      "Epoch [39], Loss: 7.9957, Test Loss: 8.0299, recall_train: 0.5197, recall_test: 0.4562 recall_val: 0.5141\n",
      "Epoch [40], Loss: 7.9955, Test Loss: 7.9781, recall_train: 0.5235, recall_test: 0.4844 recall_val: 0.4938\n",
      "Epoch [41], Loss: 8.0270, Test Loss: 8.0436, recall_train: 0.5145, recall_test: 0.4562 recall_val: 0.4984\n",
      "Epoch [42], Loss: 7.9824, Test Loss: 8.0037, recall_train: 0.5364, recall_test: 0.4891 recall_val: 0.5125\n",
      "Epoch [43], Loss: 7.9527, Test Loss: 8.0053, recall_train: 0.5241, recall_test: 0.4859 recall_val: 0.5156\n",
      "Epoch [44], Loss: 7.9673, Test Loss: 8.0007, recall_train: 0.5250, recall_test: 0.4703 recall_val: 0.5031\n",
      "Epoch [45], Loss: 7.9318, Test Loss: 8.0161, recall_train: 0.5338, recall_test: 0.4672 recall_val: 0.5094\n",
      "Epoch [46], Loss: 7.9602, Test Loss: 8.0230, recall_train: 0.5428, recall_test: 0.4734 recall_val: 0.5094\n",
      "Epoch [47], Loss: 7.9599, Test Loss: 8.0139, recall_train: 0.5327, recall_test: 0.4672 recall_val: 0.5109\n",
      "Epoch [48], Loss: 8.0178, Test Loss: 7.9914, recall_train: 0.5355, recall_test: 0.4813 recall_val: 0.5203\n",
      "Epoch [49], Loss: 7.9459, Test Loss: 7.9989, recall_train: 0.5438, recall_test: 0.4781 recall_val: 0.5375\n",
      "Epoch [50], Loss: 7.9659, Test Loss: 8.0095, recall_train: 0.5291, recall_test: 0.4734 recall_val: 0.5172\n",
      "Epoch [51], Loss: 7.9674, Test Loss: 8.0019, recall_train: 0.5352, recall_test: 0.4922 recall_val: 0.5219\n",
      "Epoch [52], Loss: 7.9724, Test Loss: 8.0109, recall_train: 0.5284, recall_test: 0.4859 recall_val: 0.5094\n",
      "Epoch [53], Loss: 7.8976, Test Loss: 7.9797, recall_train: 0.5301, recall_test: 0.5078 recall_val: 0.5031\n",
      "Epoch [54], Loss: 8.0024, Test Loss: 8.0007, recall_train: 0.5375, recall_test: 0.4828 recall_val: 0.5156\n",
      "Epoch [55], Loss: 8.0180, Test Loss: 8.0251, recall_train: 0.5356, recall_test: 0.4750 recall_val: 0.5188\n",
      "Epoch [56], Loss: 8.0038, Test Loss: 8.0098, recall_train: 0.5414, recall_test: 0.4813 recall_val: 0.5266\n",
      "Epoch [57], Loss: 8.0285, Test Loss: 7.9675, recall_train: 0.5354, recall_test: 0.4938 recall_val: 0.5141\n",
      "Epoch [58], Loss: 7.9851, Test Loss: 8.0049, recall_train: 0.5354, recall_test: 0.4625 recall_val: 0.5203\n",
      "Epoch [59], Loss: 7.9384, Test Loss: 8.0017, recall_train: 0.5344, recall_test: 0.4797 recall_val: 0.5266\n",
      "Epoch [60], Loss: 7.9756, Test Loss: 8.0116, recall_train: 0.5345, recall_test: 0.4781 recall_val: 0.5031\n",
      "Epoch [61], Loss: 7.9055, Test Loss: 7.9635, recall_train: 0.5417, recall_test: 0.4922 recall_val: 0.5031\n",
      "Epoch [62], Loss: 8.0258, Test Loss: 7.9771, recall_train: 0.5360, recall_test: 0.5000 recall_val: 0.5234\n",
      "Epoch [63], Loss: 7.9287, Test Loss: 7.9597, recall_train: 0.5423, recall_test: 0.5250 recall_val: 0.5359\n",
      "Epoch [64], Loss: 7.9778, Test Loss: 7.9904, recall_train: 0.5405, recall_test: 0.4953 recall_val: 0.5437\n",
      "Epoch [65], Loss: 7.9753, Test Loss: 7.9862, recall_train: 0.5431, recall_test: 0.4938 recall_val: 0.5406\n",
      "Epoch [66], Loss: 7.9277, Test Loss: 7.9890, recall_train: 0.5486, recall_test: 0.4891 recall_val: 0.5219\n",
      "Epoch [67], Loss: 7.9887, Test Loss: 8.0131, recall_train: 0.5473, recall_test: 0.4844 recall_val: 0.5406\n",
      "Epoch [68], Loss: 7.9915, Test Loss: 8.0321, recall_train: 0.5446, recall_test: 0.4719 recall_val: 0.5250\n",
      "Epoch [69], Loss: 7.9397, Test Loss: 8.0380, recall_train: 0.5457, recall_test: 0.4828 recall_val: 0.5406\n",
      "Epoch [70], Loss: 7.9779, Test Loss: 8.0198, recall_train: 0.5490, recall_test: 0.4813 recall_val: 0.5281\n",
      "Epoch [71], Loss: 8.0084, Test Loss: 8.0202, recall_train: 0.5490, recall_test: 0.4844 recall_val: 0.5203\n",
      "Epoch [72], Loss: 8.0153, Test Loss: 8.0144, recall_train: 0.5444, recall_test: 0.4922 recall_val: 0.5406\n",
      "Epoch [73], Loss: 7.9691, Test Loss: 8.0286, recall_train: 0.5513, recall_test: 0.4969 recall_val: 0.5328\n",
      "Epoch [74], Loss: 7.9771, Test Loss: 8.0179, recall_train: 0.5530, recall_test: 0.4750 recall_val: 0.5359\n",
      "Epoch [75], Loss: 7.9686, Test Loss: 7.9996, recall_train: 0.5452, recall_test: 0.5062 recall_val: 0.5203\n",
      "Epoch [76], Loss: 7.9723, Test Loss: 8.0161, recall_train: 0.5436, recall_test: 0.5047 recall_val: 0.5359\n",
      "Epoch [77], Loss: 7.9408, Test Loss: 8.0342, recall_train: 0.5511, recall_test: 0.4922 recall_val: 0.5484\n",
      "Epoch [78], Loss: 7.9716, Test Loss: 7.9955, recall_train: 0.5484, recall_test: 0.5141 recall_val: 0.5531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79], Loss: 7.9396, Test Loss: 7.9996, recall_train: 0.5534, recall_test: 0.5062 recall_val: 0.5469\n",
      "Epoch [80], Loss: 7.9441, Test Loss: 7.9979, recall_train: 0.5596, recall_test: 0.5125 recall_val: 0.5641\n",
      "Epoch [81], Loss: 7.9477, Test Loss: 8.0076, recall_train: 0.5512, recall_test: 0.4953 recall_val: 0.5437\n",
      "Epoch [82], Loss: 7.9541, Test Loss: 8.0082, recall_train: 0.5445, recall_test: 0.4813 recall_val: 0.5484\n",
      "Epoch [83], Loss: 8.0131, Test Loss: 7.9815, recall_train: 0.5444, recall_test: 0.5141 recall_val: 0.5625\n",
      "Epoch [84], Loss: 7.9484, Test Loss: 7.9990, recall_train: 0.5497, recall_test: 0.5344 recall_val: 0.5453\n",
      "Epoch [85], Loss: 7.9461, Test Loss: 8.0126, recall_train: 0.5481, recall_test: 0.5172 recall_val: 0.5422\n",
      "Epoch [86], Loss: 7.9178, Test Loss: 7.9996, recall_train: 0.5481, recall_test: 0.5312 recall_val: 0.5328\n",
      "Epoch [87], Loss: 7.9803, Test Loss: 8.0156, recall_train: 0.5527, recall_test: 0.5266 recall_val: 0.5578\n",
      "Epoch [88], Loss: 7.9304, Test Loss: 8.0261, recall_train: 0.5575, recall_test: 0.5078 recall_val: 0.5594\n",
      "Epoch [89], Loss: 8.0087, Test Loss: 7.9968, recall_train: 0.5571, recall_test: 0.5094 recall_val: 0.5578\n",
      "Epoch [90], Loss: 7.9176, Test Loss: 8.0242, recall_train: 0.5536, recall_test: 0.5078 recall_val: 0.5469\n",
      "Epoch [91], Loss: 7.9403, Test Loss: 8.0267, recall_train: 0.5533, recall_test: 0.4953 recall_val: 0.5672\n",
      "Epoch [92], Loss: 7.9733, Test Loss: 8.0253, recall_train: 0.5511, recall_test: 0.5047 recall_val: 0.5859\n",
      "Epoch [93], Loss: 7.9789, Test Loss: 8.0265, recall_train: 0.5488, recall_test: 0.4969 recall_val: 0.5578\n",
      "Epoch [94], Loss: 7.9677, Test Loss: 8.0466, recall_train: 0.5548, recall_test: 0.4781 recall_val: 0.5437\n",
      "Epoch [95], Loss: 7.9362, Test Loss: 8.0461, recall_train: 0.5496, recall_test: 0.4625 recall_val: 0.5484\n",
      "Epoch [96], Loss: 8.0089, Test Loss: 8.0294, recall_train: 0.5558, recall_test: 0.4938 recall_val: 0.5563\n",
      "Epoch [97], Loss: 7.9691, Test Loss: 8.0410, recall_train: 0.5545, recall_test: 0.4906 recall_val: 0.5266\n",
      "Epoch [98], Loss: 7.9889, Test Loss: 8.0481, recall_train: 0.5579, recall_test: 0.4750 recall_val: 0.5375\n",
      "Epoch [99], Loss: 7.9305, Test Loss: 8.0536, recall_train: 0.5622, recall_test: 0.4953 recall_val: 0.5625\n",
      "Epoch [100], Loss: 7.9508, Test Loss: 8.0195, recall_train: 0.5575, recall_test: 0.4969 recall_val: 0.5641\n",
      "Epoch [101], Loss: 8.0075, Test Loss: 8.0287, recall_train: 0.5534, recall_test: 0.4922 recall_val: 0.5781\n",
      "Epoch [102], Loss: 7.9378, Test Loss: 8.0288, recall_train: 0.5502, recall_test: 0.4797 recall_val: 0.5609\n",
      "Epoch [103], Loss: 7.9287, Test Loss: 8.0414, recall_train: 0.5524, recall_test: 0.4766 recall_val: 0.5609\n",
      "Epoch [104], Loss: 8.0049, Test Loss: 7.9923, recall_train: 0.5574, recall_test: 0.5094 recall_val: 0.5859\n",
      "Epoch [105], Loss: 7.9739, Test Loss: 8.0013, recall_train: 0.5578, recall_test: 0.4953 recall_val: 0.5641\n",
      "Epoch [106], Loss: 7.9523, Test Loss: 8.0003, recall_train: 0.5551, recall_test: 0.5016 recall_val: 0.5734\n",
      "Epoch [107], Loss: 7.9753, Test Loss: 7.9948, recall_train: 0.5586, recall_test: 0.4891 recall_val: 0.5625\n",
      "Epoch [108], Loss: 7.9392, Test Loss: 8.0142, recall_train: 0.5555, recall_test: 0.4875 recall_val: 0.5516\n",
      "Epoch [109], Loss: 7.9380, Test Loss: 8.0373, recall_train: 0.5572, recall_test: 0.4922 recall_val: 0.5609\n",
      "Epoch [110], Loss: 7.9235, Test Loss: 8.0089, recall_train: 0.5648, recall_test: 0.5031 recall_val: 0.5547\n",
      "Epoch [111], Loss: 8.0116, Test Loss: 7.9932, recall_train: 0.5548, recall_test: 0.5156 recall_val: 0.5563\n",
      "Epoch [112], Loss: 7.9851, Test Loss: 8.0406, recall_train: 0.5529, recall_test: 0.4969 recall_val: 0.5516\n",
      "Epoch [113], Loss: 7.9499, Test Loss: 8.0395, recall_train: 0.5609, recall_test: 0.4891 recall_val: 0.5641\n",
      "Epoch [114], Loss: 7.9268, Test Loss: 8.0324, recall_train: 0.5602, recall_test: 0.5000 recall_val: 0.5578\n",
      "Epoch [115], Loss: 7.9600, Test Loss: 8.0318, recall_train: 0.5609, recall_test: 0.5000 recall_val: 0.5531\n",
      "Epoch [116], Loss: 7.9784, Test Loss: 8.0294, recall_train: 0.5610, recall_test: 0.5047 recall_val: 0.5547\n",
      "Epoch [117], Loss: 7.9631, Test Loss: 8.0103, recall_train: 0.5637, recall_test: 0.5219 recall_val: 0.5359\n",
      "Epoch [118], Loss: 7.9415, Test Loss: 8.0353, recall_train: 0.5603, recall_test: 0.4953 recall_val: 0.5437\n",
      "Epoch [119], Loss: 7.9833, Test Loss: 8.0017, recall_train: 0.5747, recall_test: 0.5078 recall_val: 0.5563\n",
      "Epoch [120], Loss: 7.9749, Test Loss: 8.0460, recall_train: 0.5601, recall_test: 0.4953 recall_val: 0.5281\n",
      "Epoch [121], Loss: 8.0048, Test Loss: 8.0392, recall_train: 0.5617, recall_test: 0.4906 recall_val: 0.5312\n",
      "Epoch [122], Loss: 7.9482, Test Loss: 8.0295, recall_train: 0.5662, recall_test: 0.4953 recall_val: 0.5609\n",
      "Epoch [123], Loss: 7.9152, Test Loss: 8.0317, recall_train: 0.5675, recall_test: 0.4891 recall_val: 0.5531\n",
      "Epoch [124], Loss: 7.9164, Test Loss: 8.0372, recall_train: 0.5615, recall_test: 0.4875 recall_val: 0.5500\n",
      "Epoch [125], Loss: 7.8717, Test Loss: 8.0476, recall_train: 0.5656, recall_test: 0.4969 recall_val: 0.5563\n",
      "Epoch [126], Loss: 7.9998, Test Loss: 8.0513, recall_train: 0.5628, recall_test: 0.4797 recall_val: 0.5484\n",
      "Epoch [127], Loss: 7.9296, Test Loss: 8.0219, recall_train: 0.5615, recall_test: 0.5000 recall_val: 0.5813\n",
      "Epoch [128], Loss: 7.9239, Test Loss: 8.0068, recall_train: 0.5699, recall_test: 0.5000 recall_val: 0.5609\n",
      "Epoch [129], Loss: 7.9854, Test Loss: 8.0311, recall_train: 0.5575, recall_test: 0.5047 recall_val: 0.5734\n",
      "Epoch [130], Loss: 7.9918, Test Loss: 8.0506, recall_train: 0.5636, recall_test: 0.4906 recall_val: 0.5641\n",
      "Epoch [131], Loss: 7.9395, Test Loss: 8.0183, recall_train: 0.5612, recall_test: 0.4859 recall_val: 0.5656\n",
      "Epoch [132], Loss: 7.9719, Test Loss: 7.9925, recall_train: 0.5584, recall_test: 0.5094 recall_val: 0.5500\n",
      "Epoch [133], Loss: 7.9471, Test Loss: 8.0299, recall_train: 0.5649, recall_test: 0.4922 recall_val: 0.5422\n",
      "Epoch [134], Loss: 7.8981, Test Loss: 8.0116, recall_train: 0.5682, recall_test: 0.5203 recall_val: 0.5375\n",
      "Epoch [135], Loss: 7.9532, Test Loss: 8.0313, recall_train: 0.5587, recall_test: 0.4938 recall_val: 0.5312\n",
      "Epoch [136], Loss: 7.8994, Test Loss: 8.0034, recall_train: 0.5574, recall_test: 0.5047 recall_val: 0.5469\n",
      "Epoch [137], Loss: 7.9492, Test Loss: 8.0286, recall_train: 0.5647, recall_test: 0.5109 recall_val: 0.5484\n",
      "Epoch [138], Loss: 7.9489, Test Loss: 8.0218, recall_train: 0.5533, recall_test: 0.5016 recall_val: 0.5625\n",
      "Epoch [139], Loss: 7.9672, Test Loss: 8.0174, recall_train: 0.5621, recall_test: 0.5031 recall_val: 0.5672\n",
      "Epoch [140], Loss: 7.9402, Test Loss: 8.0140, recall_train: 0.5599, recall_test: 0.5047 recall_val: 0.5563\n",
      "Epoch [141], Loss: 7.9043, Test Loss: 8.0061, recall_train: 0.5669, recall_test: 0.5078 recall_val: 0.5594\n",
      "Epoch [142], Loss: 7.9930, Test Loss: 7.9879, recall_train: 0.5573, recall_test: 0.5172 recall_val: 0.5828\n",
      "Epoch [143], Loss: 7.9573, Test Loss: 8.0084, recall_train: 0.5646, recall_test: 0.5047 recall_val: 0.5687\n",
      "Epoch [144], Loss: 7.9604, Test Loss: 8.0058, recall_train: 0.5597, recall_test: 0.5125 recall_val: 0.5547\n",
      "Epoch [145], Loss: 7.9900, Test Loss: 7.9968, recall_train: 0.5601, recall_test: 0.5219 recall_val: 0.5531\n",
      "Epoch [146], Loss: 7.9514, Test Loss: 8.0253, recall_train: 0.5659, recall_test: 0.5031 recall_val: 0.5469\n",
      "Epoch [147], Loss: 7.9881, Test Loss: 8.0177, recall_train: 0.5610, recall_test: 0.5016 recall_val: 0.5328\n",
      "Epoch [148], Loss: 7.9970, Test Loss: 7.9882, recall_train: 0.5616, recall_test: 0.5328 recall_val: 0.5469\n",
      "Epoch [149], Loss: 7.9599, Test Loss: 7.9883, recall_train: 0.5648, recall_test: 0.5078 recall_val: 0.5406\n",
      "Epoch [150], Loss: 7.9347, Test Loss: 7.9982, recall_train: 0.5705, recall_test: 0.5031 recall_val: 0.5563\n",
      "Epoch [151], Loss: 7.9768, Test Loss: 8.0140, recall_train: 0.5698, recall_test: 0.5094 recall_val: 0.5281\n",
      "Epoch [152], Loss: 8.0036, Test Loss: 8.0175, recall_train: 0.5707, recall_test: 0.4844 recall_val: 0.5516\n",
      "Epoch [153], Loss: 7.9200, Test Loss: 7.9941, recall_train: 0.5707, recall_test: 0.5031 recall_val: 0.5563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8d8f288b32f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/fast-soft-sort/fast_soft_sort/pytorch_ops.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mNumpyOpWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/fast-soft-sort/fast_soft_sort/numpy_ops.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_computed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/fast-soft-sort/fast_soft_sort/numpy_ops.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_computed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misotonic_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_permutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/fast-soft-sort/fast_soft_sort/numpy_ops.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m       \u001b[0mreturn_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sum_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m def sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n\u001b[1;32m   2047\u001b[0m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(1000):\n",
    "#   Train step: get batch of train documents, then generate the dataloader containing \n",
    "#   the train queries and the correct data labels\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for step, d in enumerate(docs_loader):\n",
    "        query_loader = return_loader(d, ret='query')\n",
    "        d = d.to(device)\n",
    "        for i, (q, l) in enumerate(query_loader):  \n",
    "            q = q.to(device)\n",
    "            l = l.to(device)\n",
    "            outputs = model(d,q)\n",
    "            loss = criterion(outputs, l.squeeze())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += l.size(0)\n",
    "            train_correct += (predicted == l.flatten()).sum().item()\n",
    "        # print('Epoch [{}] Step [{}] Loss: {:.4f}'.format(epoch, step, loss.item()))\n",
    "        \n",
    "#   Eval step: repeat but with the test documents and test queries      \n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            for d in docs_test_loader:\n",
    "            # for d in docs_loader:\n",
    "                test_loader = return_loader(d, ret='test')\n",
    "                d = d.to(device)\n",
    "                for q, l in test_loader:\n",
    "                    q = q.to(device)\n",
    "                    l = l.to(device)\n",
    "                    lol = model(d,q)\n",
    "                    test_loss = criterion(lol, l.squeeze())\n",
    "                    outputs = model.evaluate(d,q)\n",
    "        #           Recall: TP / (TP + TN)\n",
    "                    predicted = outputs[0][:,0]\n",
    "                    total += l.size(0)\n",
    "                    correct += (predicted.to(device) == l.flatten()).sum().item()\n",
    "\n",
    "\n",
    "            for d in docs_val_loader:\n",
    "            # for d in docs_loader:\n",
    "                test_loader = return_loader(d, ret='val')\n",
    "                d = d.to(device)\n",
    "                for q, l in test_loader:\n",
    "                    q = q.to(device)\n",
    "                    l = l.to(device)\n",
    "                    outputs = model.evaluate(d,q)\n",
    "        #           Recall: TP / (TP + TN)\n",
    "                    predicted = outputs[0][:,0]\n",
    "                    total_val += l.size(0)\n",
    "                    correct_val += (predicted.to(device) == l.flatten()).sum().item()\n",
    "\n",
    "            print ('Epoch [{}], Loss: {:.4f}, Test Loss: {:.4f}, recall_train: {:.4f}, recall_test: {:.4f} recall_val: {:.4f}'\n",
    "                    .format(epoch+1, loss.item(), test_loss.item(), train_correct / train_total, correct / total, correct_val / total_val))\n",
    "\n",
    "# # Save the model checkpoint\n",
    "    # if epoch % 20 == 0:\n",
    "    #   torch.save(model.state_dict(), \"/content/gdrive/MyDrive/ckpt/model_epoch_{}_anchor_{}_k_{}_lr_{}.pt\".format(epoch, num_anchors, k, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b81b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
