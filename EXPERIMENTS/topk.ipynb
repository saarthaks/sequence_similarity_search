{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1W3PsVmRjs1N4yBK0uZBXZbedJRQnaBw8\n",
        "# !gdown --id 1Ci1NlbnzLNrscBxD3hdpwPE7cBkjuqkk"
      ],
      "metadata": {
        "id": "gnfjW-Unp020"
      },
      "id": "gnfjW-Unp020",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b81b43",
      "metadata": {
        "id": "e5b81b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351759d1-a903-49d0-bbe6-f1f98bc29ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf sequence_similarity_search\n",
        "!git clone https://github.com/saarthaks/sequence_similarity_search.git"
      ],
      "metadata": {
        "id": "wezcHRNPqHG0",
        "outputId": "c59c8962-fdd5-40e1-ded7-57db4f283552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wezcHRNPqHG0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sequence_similarity_search'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 250 (delta 107), reused 236 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (250/250), 2.29 MiB | 13.58 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google-research/fast-soft-sort.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjIGfRe2-oPo",
        "outputId": "4addaffa-839d-40f2-dd4b-e2030516a82d"
      },
      "id": "YjIGfRe2-oPo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fast-soft-sort'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 76 (delta 44), reused 64 (delta 33), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 fast-soft-sort/setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q6S6lgY_NfS",
        "outputId": "379fce39-550e-4b8e-d28d-e817770716d6"
      },
      "id": "4q6S6lgY_NfS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating fast_soft_sort.egg-info\n",
            "writing fast_soft_sort.egg-info/PKG-INFO\n",
            "writing dependency_links to fast_soft_sort.egg-info/dependency_links.txt\n",
            "writing requirements to fast_soft_sort.egg-info/requires.txt\n",
            "writing top-level names to fast_soft_sort.egg-info/top_level.txt\n",
            "writing manifest file 'fast_soft_sort.egg-info/SOURCES.txt'\n",
            "reading manifest file 'fast_soft_sort.egg-info/SOURCES.txt'\n",
            "writing manifest file 'fast_soft_sort.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fast_soft_sort.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fast_soft_sort.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fast_soft_sort.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fast_soft_sort.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying fast_soft_sort.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/fast_soft_sort-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing fast_soft_sort-0.1-py3.7.egg\n",
            "Copying fast_soft_sort-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding fast-soft-sort 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/fast_soft_sort-0.1-py3.7.egg\n",
            "Processing dependencies for fast-soft-sort==0.1\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.21.5\n",
            "Best match: numpy 1.21.5\n",
            "Adding numpy 1.21.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numba==0.51.2\n",
            "Best match: numba 0.51.2\n",
            "Adding numba 0.51.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for llvmlite==0.34.0\n",
            "Best match: llvmlite 0.34.0\n",
            "Adding llvmlite 0.34.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==57.4.0\n",
            "Best match: setuptools 57.4.0\n",
            "Adding setuptools 57.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for fast-soft-sort==0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchsort\n",
        "!pip3 install faiss-cpu"
      ],
      "metadata": {
        "id": "YO_mqSHnqPRS",
        "outputId": "e99b81f6-8181-4b24-d80e-ea3e5d1196b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YO_mqSHnqPRS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsort\n",
            "  Downloading torchsort-0.1.9.tar.gz (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsort) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsort) (3.10.0.2)\n",
            "Building wheels for collected packages: torchsort\n",
            "  Building wheel for torchsort (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsort: filename=torchsort-0.1.9-cp37-cp37m-linux_x86_64.whl size=2598681 sha256=096088767badbb582f2db0d99cdfa1439bb5da0d58f006da81d61b2f7aaec12b\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/20/69/cc894d3d65cb58f99ada9462cd8fb76e3beaac185efa06df3e\n",
            "Successfully built torchsort\n",
            "Installing collected packages: torchsort\n",
            "Successfully installed torchsort-0.1.9\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 4.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "id": "F2xZHbdCrBcs",
        "outputId": "a71966a0-28b4-481b-e4ba-918f07755a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "F2xZHbdCrBcs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "build  D.pt\t       fast_soft_sort.egg-info\tsample_data\n",
            "dist   fast-soft-sort  Q.pt\t\t\tsequence_similarity_search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddb0015",
      "metadata": {
        "id": "6ddb0015",
        "outputId": "c27a3c36-a953-4f1a-e305-c38eae9e9563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "sys.path.insert(1, './sequence_similarity_search/classes')\n",
        "sys.path.insert(1, './fast-soft-sort/fast_soft_sort')\n",
        "# sys.path.insert(1, '/Users/derekhuang/Documents/Research/fast-soft-sort/fast_soft_sort')\n",
        "from data_classes import BERTDataset\n",
        "from dist_perm import DistPerm\n",
        "import utils\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import torchsort\n",
        "\n",
        "def setup_logger(logger):\n",
        "    logger.setLevel(logging.INFO)\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "    log_formatter = logging.Formatter(\"[%(thread)s] %(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
        "    # console = logging.StreamHandler()\n",
        "    # console.setFormatter(log_formatter)\n",
        "    # logger.addHandler(console)\n",
        "\n",
        "\n",
        "def soft_rank(array):\n",
        "  #  return pytorch_ops.soft_rank(array.cpu(), direction=\"DESCENDING\", regularization_strength=.001).cuda()\n",
        "   return torchsort.soft_rank(-1 * array, regularization_strength=.000001)\n",
        "\n",
        "class AnchorNet(nn.Module):\n",
        "    def __init__(self, num_anchs, d, k, hidden=128, hidden2=128, out=128):\n",
        "        super(AnchorNet, self).__init__()\n",
        "        self.transform = nn.Sequential(\n",
        "          # nn.Linear(d, hidden),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(hidden, hidden2),\n",
        "          # nn.ReLU(),\n",
        "          # nn.Linear(hidden, hidden2),\n",
        "          # nn.Linear(hidden2, out)\n",
        "        )\n",
        "        self.anchors = nn.Linear(out, num_anchs)\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, data, query):\n",
        "        # data_out = self.transform(data)\n",
        "        # query_out = self.transform(query)\n",
        "        # data_rank = torch.clamp(soft_rank(self.anchors(data_out)), max=k)\n",
        "        # query_rank = torch.clamp(soft_rank(self.anchors(query_out)), max=k)\n",
        "        # data_rank = soft_rank(self.anchors(data))\n",
        "        # query_rank = soft_rank(self.anchors(query))\n",
        "        data_rank = self.anchors(data)\n",
        "        query_rank = self.anchors(query)\n",
        "        anchor_norm = torch.norm(self.anchors.weight, dim=0)\n",
        "        data_rank = soft_rank(torch.div(data_rank, anchor_norm))\n",
        "        query_rank = soft_rank(torch.div(query_rank, anchor_norm))\n",
        "        out = torch.matmul(query_rank, data_rank.T)\n",
        "        out = torch.clamp(soft_rank(out), max=k)\n",
        "        return out\n",
        "\n",
        "    def evaluate(self, data, query):\n",
        "        data_rank = self.anchors(data)\n",
        "        query_rank = self.anchors(query)\n",
        "        anchor_norm = torch.norm(self.anchors.weight, dim=0)\n",
        "        d_dist = torch.div(data_rank, anchor_norm)\n",
        "        q_dist = torch.div(query_rank, anchor_norm)\n",
        "\n",
        "        query_ranks = self.k*torch.ones(q_dist.shape, dtype=torch.float)\n",
        "        data_ranks = self.k*torch.ones(d_dist.shape, dtype=torch.float)\n",
        "\n",
        "        query_ids = torch.argsort(q_dist, dim=1)[:, :self.k]\n",
        "        data_ids = torch.argsort(d_dist, dim=1)[:, :self.k]\n",
        "\n",
        "        q_ids = torch.arange(query.shape[0])[:,None]\n",
        "        d_ids = torch.arange(data.shape[0])[:,None]\n",
        "        query_ranks[q_ids, query_ids] = torch.arange(self.k, dtype=torch.float)\n",
        "        data_ranks[d_ids, data_ids] = torch.arange(self.k, dtype=torch.float)\n",
        "\n",
        "        db_dists = torch.cdist(data_ranks, query_ranks, p=1).float()\n",
        "        closest_idx = torch.topk(db_dists, k, dim=0, largest=False)\n",
        "        return closest_idx[1].transpose(0,1), query_ranks, closest_idx[0]\n",
        "        # return closest_idx, query_ranks, closest_idx[0]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db55421",
      "metadata": {
        "id": "9db55421"
      },
      "outputs": [],
      "source": [
        "n = 200000\n",
        "D = 128\n",
        "num_queries = 3200\n",
        "num_anchors = 128\n",
        "R = 100\n",
        "k = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0c7076",
      "metadata": {
        "scrolled": true,
        "id": "2c0c7076"
      },
      "outputs": [],
      "source": [
        "file_q = './Q.pt'\n",
        "file_db = './D.pt'\n",
        "data_source = BERTDataset(file_q, file_db, n)\n",
        "db = data_source.generate_db()\n",
        "data = np.array(db).astype(np.float32)\n",
        "queries = data_source.generate_queries(num_queries)\n",
        "quers = np.array(queries).astype(np.float32)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "setup_logger(logger)\n",
        "logging.basicConfig(filename='example.log', level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7C1vXnSe77zo"
      },
      "id": "7C1vXnSe77zo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62bcac88",
      "metadata": {
        "id": "62bcac88"
      },
      "outputs": [],
      "source": [
        "import torch, itertools\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def dataset_split(dataset, train_frac):\n",
        "    length = len(dataset)\n",
        "   \n",
        "    train_length = int(length * train_frac)\n",
        "    valid_length = int((length - train_length) / 2)\n",
        "    test_length  = length - train_length - valid_length\n",
        "\n",
        "    sets = random_split(dataset, (train_length, valid_length, test_length))\n",
        "    dataset = {name: set for name, set in zip(('train', 'val', 'test'), sets)}\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2e9ce4",
      "metadata": {
        "id": "4d2e9ce4"
      },
      "outputs": [],
      "source": [
        "# Fits fine in mem\n",
        "batch_size=3200\n",
        "train_split = .8\n",
        "\n",
        "query_datasets = dataset_split(quers, train_split)\n",
        "# doc_datasets = dataset_split(db, train_split)\n",
        "doc_datasets = dataset_split(db, train_split)\n",
        "\n",
        "# The fixed train queries\n",
        "query_data_loader = torch.utils.data.DataLoader(dataset=query_datasets['train'], \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "\n",
        "# The fixed test queries \n",
        "query_data_test_loader = torch.utils.data.DataLoader(dataset=query_datasets['test'], \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# The fixed val queries \n",
        "query_data_val_loader = torch.utils.data.DataLoader(dataset=query_datasets['val'], \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# The train docs\n",
        "docs_loader = torch.utils.data.DataLoader(dataset=doc_datasets['train'], \n",
        "                                           batch_size=5000, \n",
        "                                           shuffle=False)\n",
        "\n",
        "# The test docs\n",
        "docs_test_loader = torch.utils.data.DataLoader(dataset=doc_datasets['test'], \n",
        "                                           batch_size=5000, \n",
        "                                           shuffle=False)\n",
        "\n",
        "docs_val_loader = torch.utils.data.DataLoader(dataset=doc_datasets['val'], \n",
        "                                           batch_size=5000, \n",
        "                                           shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ebd114",
      "metadata": {
        "id": "e6ebd114"
      },
      "outputs": [],
      "source": [
        "# Generate the nearest neighbors for each batch of docs \n",
        "# d = docs\n",
        "# ret = train/test\n",
        "def return_loader(d, ret):\n",
        "    index_l2 = NearestNeighbors()\n",
        "    index_l2.fit(d)\n",
        "    q = None\n",
        "#   Get the right data\n",
        "    if ret=='query':\n",
        "        q = next(iter(query_data_loader))\n",
        "    elif ret=='test':\n",
        "        q = next(iter(query_data_test_loader))\n",
        "    elif ret=='val':\n",
        "        q = next(iter(query_data_val_loader))\n",
        "#   Get the true nearest neighbors\n",
        "    true = torch.tensor(index_l2.kneighbors(q, n_neighbors=k)[1])\n",
        "    # print(true)÷\n",
        "    query_data = []\n",
        "    for i in range(q.shape[0]):\n",
        "        ground_truth = np.arange(len(d)) + 1\n",
        "        top = np.arange(k) + 1\n",
        "        # print(len(list(true[i].view(-1, 1))))\n",
        "        # print(len(top))\n",
        "        ground_truth.put(true[i].view(-1, 1), top)\n",
        "        query_data.append([q[i], ground_truth])\n",
        "    test_set = dataset_split(query_data, 1)\n",
        "#   Return data as a dataloader\n",
        "    data = torch.utils.data.DataLoader(dataset=test_set['train'], \n",
        "                                           batch_size=320, \n",
        "                                           shuffle=False)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AnchorNet(num_anchors, D, k).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "lr=.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)  "
      ],
      "metadata": {
        "id": "ZWYlWgSDA219"
      },
      "id": "ZWYlWgSDA219",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ad9f37",
      "metadata": {
        "id": "46ad9f37"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for d in docs_test_loader:\n",
        "    # for d in docs_loader:\n",
        "        test_loader = return_loader(d, ret='test')\n",
        "        d = d.to(device)\n",
        "        for q, l in test_loader:\n",
        "#             print(q)\n",
        "            q = q.to(device)\n",
        "            l = l.to(device)\n",
        "            outputs = model.evaluate(d,q)\n",
        "#           Recall: TP / (TP + TN)\n",
        "            predicted = outputs[0][:,2]\n",
        "            # print(predicted)\n",
        "            # print(predicted./shape)\n",
        "            # total += l.size(0)\n",
        "            # correct += (predicted.to(device) == l.flatten()).sum().item()\n",
        "\n",
        "    # print ('recall_test: {:.4f}'\n",
        "            # .format(correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583566b1",
      "metadata": {
        "scrolled": true,
        "id": "583566b1",
        "outputId": "9e0a9248-b685-4e96-9ab6-093a505382d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Loss: 8283144.0000, Test Loss: 8303299.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [2], Loss: 8282185.0000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [3], Loss: 8282191.0000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [4], Loss: 8282654.5000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [5], Loss: 8282615.0000, Test Loss: 8283142.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [6], Loss: 8282796.0000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [7], Loss: 8282056.5000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [8], Loss: 8282269.0000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n",
            "Epoch [9], Loss: 8282534.5000, Test Loss: 8283143.5000, recall_train: 1.0000, recall_test: 1.0000 recall_val: 1.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-edf08257f72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mquery_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-db1d1c1cd202>\u001b[0m in \u001b[0;36mreturn_loader\u001b[0;34m(d, ret)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mquery_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print(len(list(true[i].view(-1, 1))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_steps = []\n",
        "for epoch in range(150):\n",
        "#   Train step: get batch of train documents, then generate the dataloader containing \n",
        "#   the train queries and the correct data labels\n",
        "    train_correct = 1\n",
        "    train_total = 1\n",
        "    for step, d in enumerate(docs_loader):\n",
        "        query_loader = return_loader(d, ret='query')\n",
        "        d = d.to(device)\n",
        "        for i, (q, l) in enumerate(query_loader):  \n",
        "            q = q.to(device)\n",
        "            l = l.to(device)\n",
        "            outputs = model(d,q)\n",
        "            loss = criterion(outputs, l.squeeze().float())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # train_total += l.size(0)\n",
        "            # train_correct += (predicted == l.flatten()).sum().item()\n",
        "        # print('Epoch [{}] Step [{}] Loss: {:.4f}'.format(epoch, step, loss.item()))\n",
        "        \n",
        "#   Eval step: repeat but with the test documents and test queries      \n",
        "    if epoch % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            correct = 1\n",
        "            total = 1\n",
        "            correct_val = 1\n",
        "            total_val = 1\n",
        "            for d in docs_test_loader:\n",
        "            # for d in docs_loader:\n",
        "                test_loader = return_loader(d, ret='test')\n",
        "                d = d.to(device)\n",
        "                for q, l in test_loader:\n",
        "                    q = q.to(device)\n",
        "                    l = l.to(device)\n",
        "                    lol = model(d,q)\n",
        "                    test_loss = criterion(lol, l.squeeze())\n",
        "                    outputs = model.evaluate(d,q)\n",
        "        #           Recall: TP / (TP + TN)\n",
        "                    predicted = outputs[0][:,0]\n",
        "                    # total += l.size(0)\n",
        "                    # correct += (predicted.to(device) == l.flatten()).sum().item()\n",
        "\n",
        "\n",
        "            for d in docs_val_loader:\n",
        "            # for d in docs_loader:\n",
        "                test_loader = return_loader(d, ret='val')\n",
        "                d = d.to(device)\n",
        "                for q, l in test_loader:\n",
        "                    q = q.to(device)\n",
        "                    l = l.to(device)\n",
        "                    outputs = model.evaluate(d,q)\n",
        "        #           Recall: TP / (TP + TN)\n",
        "                    predicted = outputs[0][:,0]\n",
        "                    # total_val += l.size(0)\n",
        "                    # correct_val += (predicted.to(device) == l.flatten()).sum().item()\n",
        "\n",
        "            # loss_steps.append(loss.item())\n",
        "            # plt.plot(loss_steps)\n",
        "            # plt.savefig('test.png')\n",
        "\n",
        "            print ('Epoch [{}], Loss: {:.4f}, Test Loss: {:.4f}, recall_train: {:.4f}, recall_test: {:.4f} recall_val: {:.4f}'\n",
        "                    .format(epoch+1, loss.item(), test_loss.item(), train_correct / train_total, correct / total, correct_val / total_val))\n",
        "\n",
        "# # # Save the model checkpoint\n",
        "#     if epoch % 20 == 0:\n",
        "#       torch.save(model.state_dict(), \"/content/gdrive/MyDrive/ckpt/model_epoch_{}_anchor_{}_k_{}_lr_{}.pt\".format(epoch, num_anchors, k, lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qA3Ych09MgBa"
      },
      "id": "qA3Ych09MgBa"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/MyDrive/'Colab Notebooks'/"
      ],
      "metadata": {
        "id": "g6c9hqiaZYuT"
      },
      "id": "g6c9hqiaZYuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pycat code.py"
      ],
      "metadata": {
        "id": "cyR4nydeZrUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8077ba99-7e0d-4b3e-dc2b-619ce26430bb"
      },
      "id": "cyR4nydeZrUL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: no such file, variable, URL, history range or macro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XGlJfAwjhD0S"
      },
      "id": "XGlJfAwjhD0S",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Copy of diffsort.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}